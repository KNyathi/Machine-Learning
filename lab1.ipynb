{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ньяти Каелиле БВТ2201 Лаб 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.3.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: gensim in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\khaye\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn gensim nltk numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0.         0.         0.         0.         0.         0.57735027\n",
      "  0.57735027 0.57735027 0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.4472136  0.4472136  0.4472136  0.\n",
      "  0.         0.         0.4472136  0.4472136  0.         0.\n",
      "  0.        ]\n",
      " [0.4472136  0.4472136  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.4472136  0.4472136\n",
      "  0.4472136 ]]\n",
      "Vocabulary: ['data' 'enjoy' 'fascinating' 'is' 'language' 'learning' 'love' 'machine'\n",
      " 'natural' 'processing' 'text' 'with' 'working']\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF = TF * IDF (formula)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "# change this and use kaggle dataset\n",
    "documents = [\n",
    "    \"I love machine learning.\",\n",
    "    \"Natural language processing is fascinating.\",\n",
    "    \"I enjoy working with text data.\"\n",
    "]\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to array (optional)\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_array)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khaye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'king' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m vector \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwv[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmachine\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Get vector for a word\u001b[39;00m\n\u001b[0;32m     42\u001b[0m similar_words \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mmost_similar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmachine\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Find similar words\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mking\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwoman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Word analogy\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVector for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmachine\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, vector)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWords similar to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmachine\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, similar_words)\n",
      "File \u001b[1;32mc:\\Users\\khaye\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\khaye\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'king' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.data.path.append('C:\\\\Users\\\\khaye\\\\AppData\\\\Roaming\\\\nltk_data')\n",
    "\n",
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "'''\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"I love machine learning.\",\n",
    "    \"Natural language processing king queen woman man is fascinating.\",\n",
    "    \"I enjoy working with text data.\"\n",
    "]\n",
    "\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "'''\n",
    "\n",
    "# Load a larger dataset from kaggle (20 Newsgroups)\n",
    "encoding_to_try = 'ISO-8859-1'  # or 'latin1'\n",
    "\n",
    "with open(\"archive/sci.electronics.txt\", \"r\", encoding=encoding_to_try) as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in nltk.sent_tokenize(text)]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model (optional)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Load the model (optional)\n",
    "# model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Perform word vector operations\n",
    "vector = model.wv['machine']  # Get vector for a word\n",
    "similar_words = model.wv.most_similar('machine')  # Find similar words\n",
    "result = model.wv.most_similar(positive=['king', 'woman'], negative=['man'])  # Word analogy\n",
    "\n",
    "print(\"Vector for 'machine':\", vector)\n",
    "print(\"Words similar to 'machine':\", similar_words)\n",
    "print(\"king - man + woman ≈\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Vector: [-1.07072602e-03  5.62718725e-04 -3.66303029e-04 -6.00932041e-04\n",
      " -4.90840310e-05  1.45265725e-05  3.71889341e-04  5.66103881e-04\n",
      " -2.01790934e-04 -3.17921396e-04 -8.54524461e-05  3.12310061e-04\n",
      " -5.10916812e-04 -6.13339415e-04  3.28334013e-05 -6.55545423e-04\n",
      " -1.43303858e-05 -2.38833071e-04 -2.07630953e-04  6.34699021e-06\n",
      " -2.89229170e-04 -5.07601284e-04  1.13142497e-03  1.17431158e-03\n",
      " -8.75906499e-04 -1.60494170e-05  4.45088034e-04 -9.78748386e-06\n",
      " -2.26803276e-04  3.34837570e-04  5.90206218e-04 -4.59753057e-04\n",
      " -4.67774409e-04 -8.68298713e-04  5.53022552e-04  5.24864210e-04\n",
      "  5.43601139e-04  1.14810904e-04 -2.27131200e-04  3.52305839e-04\n",
      "  6.55690468e-04 -4.00557022e-04 -4.75774555e-04  1.76355323e-04\n",
      "  5.65900857e-04 -3.24149276e-04  2.52342078e-04  2.36025984e-04\n",
      "  4.12129202e-05 -4.93635598e-04  4.90649567e-04 -1.74786504e-04\n",
      "  3.65458035e-04 -2.48845935e-04 -7.60394511e-04 -2.22041131e-04\n",
      "  9.79078468e-05 -3.24830305e-04 -1.06824150e-04 -8.22968548e-04\n",
      "  6.00620515e-04 -8.35383096e-05  8.59892318e-04 -3.80296314e-04\n",
      "  2.07595223e-04 -1.84688144e-04  4.40458203e-04  8.69294682e-04\n",
      " -6.34685078e-04 -4.41397684e-04  1.10676879e-04  3.61934430e-04\n",
      "  1.62794904e-04  8.50399795e-04 -7.31032252e-07  4.64081281e-04\n",
      " -7.56754513e-04  8.51016904e-04  9.53653653e-05 -1.58848545e-04\n",
      " -2.61054965e-04 -3.66055604e-04  5.71356692e-04 -4.31262548e-04\n",
      " -4.09699403e-04 -8.68826155e-04  5.41973931e-04  7.32590426e-04\n",
      " -5.92840943e-04 -5.04009005e-04  2.12374573e-04  1.40393171e-04\n",
      " -1.65323760e-04 -4.35618057e-04  9.86430516e-04 -1.50989381e-04\n",
      "  4.66124186e-04 -5.02770820e-04  5.51873617e-05 -4.09133204e-04]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get TF-IDF weights for a document\n",
    "document_index = 0  # Choose a document\n",
    "tfidf_weights = tfidf_array[document_index]\n",
    "\n",
    "# Get Word2Vec embeddings for words in the document\n",
    "words = vectorizer.get_feature_names_out()\n",
    "weighted_embeddings = np.zeros((len(words), model.vector_size))\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    if word in model.wv:\n",
    "        weighted_embeddings[i] = tfidf_weights[i] * model.wv[word]\n",
    "\n",
    "# Average the weighted embeddings to get a document vector\n",
    "document_vector = np.mean(weighted_embeddings, axis=0)\n",
    "\n",
    "print(\"Document Vector:\", document_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
