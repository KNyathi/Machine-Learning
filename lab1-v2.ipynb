{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn 20newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn gensim nltk numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khaye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khaye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khaye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "TF-IDF Matrix Shape: (1000, 18204)\n",
      "Vocabulary: ['aa' 'aaa' 'aacc' 'aadnsi' 'aamir' 'aaph' 'aapjj' 'aapp' 'aaron'\n",
      " 'aausmausmaedu' 'aaverage' 'aaxclear' 'aaxdeoofsqfjmxhhls' 'ab'\n",
      " 'ababspalestinians' 'abandoned' 'abates' 'abberant' 'abbreviation' 'abc']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load dataset (using only first 100 rows for efficiency)\n",
    "dataset = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "texts = dataset.data[:1000]  \n",
    "\n",
    "# Preprocessing function\n",
    "stopwords_list = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization & Lowercasing\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords_list]  # Remove stopwords & lemmatize\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "cleaned_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(cleaned_texts)\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Print results\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_array)\n",
    "print(\"TF-IDF Matrix Shape:\", tfidf_array.shape)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out()[:20])  # Print first 20 words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'man' exists in the vocabulary\n",
      "'woman' exists in the vocabulary\n",
      "'wife' exists in the vocabulary\n",
      "Vector for 'wife': [-0.07911736  0.072822   -0.80405515 -0.7014165   0.05498299 -0.05689457\n",
      " -0.5800458  -0.0318243  -0.04190193  0.75361246]\n",
      "Words similar to 'wife': [('liturgy', 0.368179053068161), ('torture', 0.358430415391922), ('joseph', 0.35421454906463623), ('sunday', 0.3444647789001465), ('convenient', 0.3442094027996063), ('execution', 0.3381751477718353), ('employee', 0.3375227153301239), ('daughter', 0.3373296558856964), ('classic', 0.33692508935928345), ('dear', 0.3363684117794037)]\n",
      "king - man + woman ≈ [('aged', 0.3783465325832367), ('abusive', 0.35764965415000916), ('seeking', 0.3521312177181244), ('massacred', 0.3485230505466461), ('agdam', 0.3473351299762726), ('effendi', 0.3434714376926422), ('azeri', 0.3377446234226227), ('prevalence', 0.3314957320690155), ('morgue', 0.33065593242645264), ('median', 0.32908132672309875)]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Prepare data for Word2Vec\n",
    "tokenized_texts = [text.split() for text in cleaned_texts]\n",
    "\n",
    "# Train Word2Vec model\n",
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(min_count=4,\n",
    "                         window=4,\n",
    "                         vector_size=300, \n",
    "                         alpha=0.03, \n",
    "                         min_alpha=0.0007, \n",
    "                         sg = 1,\n",
    "                         workers=cores-1)\n",
    "\n",
    "w2v_model.build_vocab(tokenized_texts, progress_per=10000)\n",
    "w2v_model.train(tokenized_texts, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)\n",
    "\n",
    "# Save the model \n",
    "w2v_model.save(\"word2vec2.model\")\n",
    "\n",
    "\n",
    "# Test Word2Vec\n",
    "word = \"computer\"\n",
    "\n",
    "result = w2v_model.wv.most_similar(positive=['husband', 'woman'], negative=['man'])\n",
    "\n",
    "if word in w2v_model.wv:\n",
    "    print(f\"Vector for '{word}':\", w2v_model.wv[word][:10])  \n",
    "    print(f\"Words similar to '{word}':\", w2v_model.wv.most_similar(positive=[word]))\n",
    "    print(\"husband - man + woman ≈\", result)\n",
    "else:\n",
    "    print(f\"'{word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khaye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khaye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khaye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar documents:\n",
      "Document 979 - Similarity Score: 0.9987\n",
      "Document 491 - Similarity Score: 0.9983\n",
      "Document 567 - Similarity Score: 0.9981\n",
      "Document 384 - Similarity Score: 0.9978\n",
      "Document 843 - Similarity Score: 0.9977\n",
      "Most relevant words to the first document: [('start', 0.9987203478813171), ('havent', 0.998412549495697), ('wont', 0.9981712102890015), ('sell', 0.9979532957077026), ('driver', 0.9979207515716553), ('mine', 0.9979206323623657), ('solution', 0.9978811740875244), ('heard', 0.9978165626525879), ('detail', 0.9977374076843262), ('perhaps', 0.9975965023040771)]\n",
      "husband - man + woman ≈ [('national', 0.9843640923500061), ('april', 0.9805154204368591), ('center', 0.9789007306098938), ('russian', 0.9752913117408752), ('army', 0.9752044081687927)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "texts = dataset.data[:1000]  # Using a subset for faster training\n",
    "\n",
    "# Preprocessing function\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    tokens = word_tokenize(text.lower())  # Lowercase and tokenize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Train Word2Vec model\n",
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(sentences=tokenized_texts,\n",
    "                      min_count=4,\n",
    "                         window=4,\n",
    "                         vector_size=300, \n",
    "                         alpha=0.03, \n",
    "                         min_alpha=0.0007, \n",
    "                         sg = 1,\n",
    "                         workers=cores-1)\n",
    "# Train TF-IDF model\n",
    "corpus = [\" \".join(tokens) for tokens in tokenized_texts]  # Convert tokenized text back to sentences\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "tfidf_vocab = tfidf_vectorizer.vocabulary_\n",
    "\n",
    "# Function to compute TF-IDF weighted Word2Vec vectors\n",
    "def get_weighted_w2v_vector(text):\n",
    "    tokens = preprocess_text(text)\n",
    "    vector = np.zeros(300)  # Initialize empty vector of same size as Word2Vec embeddings\n",
    "    total_weight = 0\n",
    "\n",
    "    for word in tokens:\n",
    "        if word in w2v_model.wv and word in tfidf_vocab:\n",
    "            tfidf_weight = tfidf_matrix[0, tfidf_vocab[word]]  # Get TF-IDF weight\n",
    "            vector += w2v_model.wv[word] * tfidf_weight  # Weighted vector sum\n",
    "            total_weight += tfidf_weight\n",
    "\n",
    "    if total_weight > 0:\n",
    "        vector /= total_weight  # Normalize\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "# Compute vectors for all documents\n",
    "doc_vectors = np.array([get_weighted_w2v_vector(text) for text in texts])\n",
    "\n",
    "# Compute similarity between the first document and all others\n",
    "similarities = cosine_similarity([doc_vectors[0]], doc_vectors)[0]\n",
    "\n",
    "# Rank documents by similarity\n",
    "sorted_indices = np.argsort(similarities)[::-1]  # Sort in descending order\n",
    "\n",
    "print(\"Top 5 most similar documents:\")\n",
    "for idx in sorted_indices[1:6]:  # Skip the first one (itself)\n",
    "    print(f\"Document {idx} - Similarity Score: {similarities[idx]:.4f}\")\n",
    "\n",
    "\n",
    "# Find the most similar words to the first document vector\n",
    "similar_words = w2v_model.wv.similar_by_vector(doc_vectors[0], topn=10)  \n",
    "print(\"Most relevant words to the first document:\", similar_words)\n",
    "\n",
    "\n",
    "# Word analogy: husband - man + woman ≈ ?\n",
    "try:\n",
    "    analogy_vector = w2v_model.wv['husband'] - w2v_model.wv['man'] + w2v_model.wv['woman']\n",
    "    analogy_result = w2v_model.wv.similar_by_vector(analogy_vector, topn=5)\n",
    "    print(\"husband - man + woman ≈\", analogy_result)\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e} (Some words may be missing from vocabulary)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
